---
title: "R Tutorial 011: Interpreting linear models"
author: "RichardOnData"
date: "12/9/2020"
output: 
  html_document:
    code_folding: "show"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```

```{r Load libraries and data}
library(tidyverse)
library(broom)
library(AER)

mtcars <- mtcars
data(CreditCard)

options(digits = 4)
options(scipen = 4)
```

This tutorial continues ideas introduced in R Tutorial 010 on creating linear models, focused on the interpretation of results from the regression methods.   Machine learning is outside of the scope of this tutorial.   We'll use the "mtcars" dataset for our linear regression example and the "CreditCard" dataset from the AER package for our logistic regression example.

Documentation on the datasets:

  * "mtcars": https://rstudio-pubs-static.s3.amazonaws.com/61800_faea93548c6b49cc91cd0c5ef5059894.html
  * "CreditCard": https://rdrr.io/cran/AER/man/CreditCard.html
  
<br>
  
### High level idea behind regression ###

The idea behind regression is completely analogous to the slope-intercept equation that you might remember from high school: $y = mx + b$, where the change (or "rise") in y is explained by the change (or "run") in x, where "m" is the slope and "b" is the intercept.

Regression methods in general are used to fit a linear model, explaining variation in a response variable linearly in terms of other variables (known as independent or explanatory variables, or predictors).   It is used to create a linear equation based on the least-squares solution - that is, through minimizing the sum of the squared differences between fitted values and actual values.

The least squares equation for the estimate of the slope parameters is: $\hat{\beta} = (X^{T}X)^{-1} X^{T}y$

where $\beta$ is the vector of true slope parameters, $\hat{\beta}$ is a vector containing the slope estimates, X is the matrix of predictors, and y is the response vector.

Don't worry if you don't knew linear algebra; there is still plenty that you can understand!

The assumption (based on a frequentist interpretion, not a Bayesian one), is that there is a true slope parameter in the population describing change in the response variable in terms of a change in an independent variable, holding all of the other predictors fixed.   

Basically, there is a true equation $y = \beta_{0} + \beta_{1}X_{1} + ... + \beta_{p}X_{p}$
but we create $\hat{y} = \hat{\beta}_{0} + \hat{\beta}_{1}X_{1} + ... + \hat{\beta}_{p}X_{p}$ where p is the number of predictors.

The regression output will return estimates of each of these slope parameters, with corresponding standard errors, test statistics (t-value for linear regression, z-value for logistic regression) and p-values, testing a hypothesis of the true slope parameter being different from zero.

There are two distinct goals one can aim to accomplish with regression: prediction, or inference.  Once the regression equation is created, it can be used to predict the value when new data are added where the response is missing.   As a predictive method, regression methods tend not to perform as well as machine learning methods; however, their interpretability is a major asset.

Distinct from this, however, is the fact that estimates of slope parameters can be used to estimate how much the response variable will change per one-unit increase in each continuous explanatory variable, or per change in levels in categorical variables.   This is the greatest appeal of regression methods; they explain variation in the response variable in an understandable way, while controlling for the effects of other variables.

<br>

### Linear regression output ###

```{r Change "mtcars" columns to factors}
mtcars <- mtcars %>%
  mutate(across(.cols = c("cyl", "vs", "am", "gear"), .fns = factor))
```

```{r Fit modified lm for "mtcars"}
formula <- "mpg ~ cyl + sqrt(disp) + hp + drat + wt + qsec + hp:wt"
linReg <- lm(formula, data = mtcars)
summary(linReg)
```

**Residuals:** Estimates of true population errors, calculated as observed values - predicted values $y - \hat{y}$

**Coefficients:** This table provides the estimates of the slope parameters as well as the corresponding standard errors of these estimates.    Each of these include t-values, testing a hypothesis that the true value of the slope parameter for that variable is significantly different than zero.   A rejection of this null hypothesis is to conclude that there is a nonzero relationship between the response variable and this predictor, holding all others variables constant.

Note some special terms in this example.   One is the sqrt(disp) term.   Per one-unit change in the square root of the "disp" variable, the response variable "mpg" is expected to change by `r linReg$coefficients[4]`.

We should also pay to the attention to the "cyl" variable.   It is important to understand that with factor variables, the model takes a "baseline" (cyl == 4 here).  For example, a car with cyl = 8 is expected to have a mpg that is `r linReg$coefficients[3]` greater than the car with cyl = 4, holding all other variables constant.

Additionally there is an interaction term for "hp:wt".   These terms could be added to the model if we believe the joint effect of "hp" and "wt" are not additive.   For example here, for one unit increases in "hp" and "wt", we would expect mpg to change by `r linReg$coefficients[5]` + `r linReg$coefficients[7]` + `r linReg$coefficients[9]` = `r linReg$coefficients[5] + linReg$coefficients[7] + linReg$coefficients[9]`.

**Residual standard error:**  This is an estimate of the standard deviation of the true population errors, and is equal to Root Mean Squared Error (RMSE) in linear regression models.   When the goal is prediction, it is a common goal to try to minimize this number.  This is equal to the Residual Sum of Squares divided by its degrees of freedom.

**Multiple R-squared:** This quantity, known as the "coefficient of determination", is the proportion of the variation in the response variable that can be explained by the linear regression on the predictors.   Please note that this quantity will increase no matter how many variables are added to the model; as a result, it is not necessarily indicative of an improved model, from the standpoint of either goodness of fit or predictive accuracy.

**Adjusted R-squared:** Adjusted R-squared corrects for the issue with the "coefficient of determination" where the more predictors that are added, the higher the quantity.   Essentially the formula for it adds a penalty factor, where it will go down as extra predictors are added.

Adjusted $R^2$ = 1 - $\frac{(1 - R^2)(n - 1)}{(n - p - 1)}$  where n is the total sample size and p is the number of predictors.

**F-statistic:** This statistic tests whether or not the linear regression model fits the data better than a model with no predictors.   A high F-statistic and corresponding low p-value indicate that the model does fit the data better than the model with no predictors.

<br>

### Logistic regression output ###

Logistic regression output is a little bit more challenging because it is more mathematically complex by nature, but the highest level is still interpretable by many stakeholders.   It is similar to linear regression; however, instead of a continuous variable for the response, it is used in the case of categorical variables.   Usually, when people refer to "logistic regression" they are specifically referring to binomial logistic regression, where the response variable has only two levels (commonly referred to as "success" vs. "failure", or "1 vs 0"), where the probability of success is defined as "p".

It is important to be familiar with the definition of 'odds' - this is a ratio of relative probabilities, that is, $\frac{p}{1-p}$.  For example, the odds of rolling a 6 in a six-sided die is 1/5.  

A key assumption from linear regression was that the continuous variable is defined across the whole spectrum of continuous numbers - that is, ( $-\infty , \infty$ ).  This is obviously not the case with probabilities which are defined from (0 to 1).   However, the log of the odds function IS defined from ( $-\infty , \infty$ )!   This is the basis for logistic regression.   This "log odds" function is known as the logit of p.

Our regression equation here is: $ln(\frac{p}{1-p}) = \hat{\beta}_{0} + \hat{\beta}_{1}X_{1} + ... + \hat{\beta}_{p}X_{p}$

A new concept with logistic regression and other GLMs is "deviance".  This indicates the degree to which the likelihood of the saturated model exceeds the likelihood of the model being used, where the saturated model is a model with as many parameters as observations, that is intended to represent "perfection".  Lower deviance indicates a higher quality of fit.

```{r Fix issues with the variables}
CreditCard$share <- CreditCard$share * 100
CreditCard$majorcards <- factor(CreditCard$majorcards)
```

```{r Fit logistic regression to "CreditCard" data}
formula <- "card ~ reports + age + income + share + owner + selfemp + dependents + months + majorcards + active"
logReg <- glm(formula, family = "binomial", data = CreditCard)
summary(logReg)
```

**Deviance Residuals:** These are analogous to the residuals from linear regression, where each observation contributes a "deviance residual" to the total deviance.

**Coefficients:** The interpretation for this table is very similar to that from the linear regression.   The estimates for slope parameters and corresponding standard errors, z-statistics, and p-values are provided.   For example, for every one-unit change in the "income" variable, the log odds of a credit card being issued increase by `r logReg$coefficients[4]`.

**Null and residual deviance:** The null deviance indicates the goodness of fit for a model including only the intercept.  The residual deviance indicates the goodness of fit for the model we created; again, the lower this number the better.

**AIC:** This quantity, the Akaike information criterion, describes the quality of the model fit.   It increases as parameters are added to the model.   The lower this quantity, the better the model fit.


There is much more to this field and the practice of linear modeling, in particular with respect to: model selection, adding weights to the model, other types of GLMs, and much more.   You should emphasize understanding the fundamentals of these methods and being able to implement them in R (or Python if that is your cup of tea) first!